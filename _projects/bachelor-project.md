---
layout: default
title: "üöó Taking the Driving Theory Test with Vision-Language Models"
rank: 10
year: 2024
description: >
  This project explores the capabilities of Vision-Language Models (VLMs) in interpreting and answering driving theory test questions, which often combine visual inputs (e.g., road signs, traffic situations) with linguistic cues. The focus was on evaluating zero-shot and few-shot performance of multimodal transformers in high-stakes reasoning tasks.
supervised_by: ["Prof. Alexandre Alahi", "Dr. Charles Corbi√®re"]
report: ../projects/bachelor-project/Bachelor_Project_Report_Matthias_Wyss.pdf
team: ["Matthias Wyss"]
location: "EPFL ‚Äì Bachelor in Communication Systems, Year 3 (2024)"
tools: [Python, PyTorch, HuggingFace Transformers, CLIP, BLIP]
techniques: [Vision-Language Modeling, Zero-Shot Learning, Fine-Tuning, Educational AI]
---

# üöó Taking the Driving Theory Test with Vision-Language Models

**üìç EPFL ‚Äì Bachelor in Communication Systems, Year 3 (2024)**  
**üìö Supervised by:** Prof. Alexandre Alahi, Dr. Charles Corbi√®re  
**üîó Final Report:** [Report](../../projects/bachelor-project/Bachelor_Project_Report_Matthias_Wyss.pdf)

---

This project explores the capabilities of **Vision-Language Models (VLMs)** in interpreting and answering driving theory test questions, which often combine **visual inputs** (e.g., road signs, traffic situations) with **linguistic cues**.

The focus was on evaluating **zero-shot** and **few-shot performance** of multimodal transformers in high-stakes reasoning tasks. By fine-tuning open models on **driving-related datasets**, we assessed their **generalization**, **interpretability**, and **real-world usability** in an educational context.

---

**üõ† Tools & Libraries:**  
- Python  
- PyTorch  
- HuggingFace Transformers  
- CLIP  
- BLIP

**üß† Techniques:**  
- Vision-Language Modeling  
- Zero-Shot Learning  
- Fine-Tuning  
- Educational AI
